{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b989e59-af2a-43ff-9774-dde3012f956c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d611c998-a542-46f5-b808-27315965b7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python\n",
    "# OR dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0debb97e-164b-4201-8a01-1d890181e321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "kafka_username = dbutils.secrets.get(scope=\"kafka-secrets\", key=\"kafka-username\")\n",
    "kafka_password = dbutils.secrets.get(scope=\"kafka-secrets\", key=\"kafka-password\")\n",
    "\n",
    "# Kafka configuration\n",
    "conf = {\n",
    "    'bootstrap.servers': 'pkc-ldvj1.ap-southeast-2.aws.confluent.cloud:9092',\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanisms': 'PLAIN',\n",
    "    'sasl.username': kafka_username,\n",
    "    'sasl.password': kafka_password\n",
    "}\n",
    "\n",
    "# Create Producer instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Function to generate mock market data\n",
    "def generate_market_data():\n",
    "    return {\n",
    "        'symbol': random.choice(['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']),\n",
    "        'price': round(random.uniform(100, 1500), 2),\n",
    "        'volume': random.randint(1, 1000),\n",
    "        'timestamp': int(time.time())\n",
    "    }\n",
    "\n",
    "# Function to send data to Kafka\n",
    "def send_to_kafka(topic, data):\n",
    "    producer.produce(topic, key=data['symbol'], value=json.dumps(data))\n",
    "    producer.flush()\n",
    "\n",
    "# Generate and send mock data\n",
    "topic = 'market_data'\n",
    "for _ in range(100):\n",
    "    data = generate_market_data()\n",
    "    send_to_kafka(topic, data)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4a17cd-1d38-4ebe-b7a7-b582a7f714b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# kafka_bootstrap_servers_tls = \"pkc-ldvj1.ap-southeast-2.aws.confluent.cloud:9092\"\n",
    "# kafka_username = dbutils.secrets.get(scope=\"kafka-secrets\", key=\"kafka-username\")\n",
    "# kafka_password = dbutils.secrets.get(scope=\"kafka-secrets\", key=\"kafka-password\")\n",
    "# topic = \"market_data\"\n",
    "# # Read a small batch of data from the Kafka topic\n",
    "# sample_df = (\n",
    "#     spark.read\n",
    "#         .format(\"kafka\")\n",
    "#         .option(\"subscribe\", topic)\n",
    "#         .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers_tls)\n",
    "#         .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "#         .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "#         .option(\"kafka.sasl.jaas.config\", f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_username}\" password=\"{kafka_password}\";')\n",
    "#         .option(\"startingOffsets\", \"earliest\")\n",
    "#         .option(\"endingOffsets\", \"latest\")\n",
    "#         .load()\n",
    "# )\n",
    "\n",
    "# # Select and cast the key and value columns to string\n",
    "# sample_df = sample_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# # Display the sample data\n",
    "# display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8f5836-dbe7-43cc-bf70-979c6a37922c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Infer the schema from the sample data\n",
    "# from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# # Assuming the value column contains JSON data\n",
    "# json_sample_df = sample_df.select(\"value\")\n",
    "# json_schema = spark.read.json(json_sample_df.rdd.map(lambda row: row.value)).schema\n",
    "\n",
    "# # Display the inferred schema\n",
    "# print(json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd30182-f55b-415b-bce1-5b910ee7b6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "\n",
    "# behavioral_input_schema = StructType([\n",
    "#     StructField(\"symbol\", StringType(), True),\n",
    "#     StructField(\"price\", DoubleType(), True),\n",
    "#     StructField(\"volume\", IntegerType(), True),\n",
    "#     StructField(\"timestamp\", LongType(), True)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c623a5b8-6fa4-44bf-89f3-4e5546d6b8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1be7d9-ddf7-4ade-8a16-4ea2f32eac22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data generator",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
